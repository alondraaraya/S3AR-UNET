{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e845d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.9.0-cp310-cp310-manylinux_2_28_x86_64.whl (899.8 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.5.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.3 MB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached torchvision-0.24.0-cp310-cp310-manylinux_2_28_x86_64.whl (8.0 MB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading polars-1.35.2-py3-none-any.whl (783 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m783.6/783.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading polars_runtime_32-1.35.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0ma \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, triton, sympy, polars-runtime-32, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, filelock, polars, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, ultralytics-thop, torchvision, ultralytics\n",
      "\u001b[2K  Attempting uninstall: typing-extensionsm\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/27\u001b[0m [mpmath]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.5.0━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/27\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.5.0:8;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/27\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.5.0━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/27\u001b[0m [mpmath]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/27\u001b[0m [ultralytics]37m━\u001b[0m \u001b[32m26/27\u001b[0m [ultralytics]ver-cu12]]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.13.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed filelock-3.20.0 fsspec-2025.10.0 mpmath-1.3.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 polars-1.35.2 polars-runtime-32-1.35.2 sympy-1.14.0 torch-2.9.0 torchvision-0.24.0 triton-3.5.0 typing-extensions-4.15.0 ultralytics-8.3.227 ultralytics-thop-2.0.18\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd144ae2-2ef3-4788-b5cf-3088212db687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from ultralytics import YOLO\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46654832-7e12-453a-9f6a-ef8aab183bc6",
   "metadata": {},
   "source": [
    "## Segmentación en frame completo + filtrado con ROI YOLO\n",
    "\n",
    "**Proceso del código:**\n",
    "\n",
    "1) Obtiene una **región de interés (ROI) global** recorriendo todo el videocon **YOLO**.  \n",
    "2) Para **cada frame completo del video**, aplica **UNet** para generar la segmentación.  \n",
    "3) Filtra la segmentación de UNet usando la ROI de YOLO, **eliminando la segmentación fuera de la zona relevante**.  \n",
    "4) Dibuja los contornos de la segmentación sobre el **video original completo**, manteniendo la resolución original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0158dc9-7083-4596-b3f7-b6878a357c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#   1) FUNCIÓN PARA OBTENER LA MÁSCARA UNET #\n",
    "#############################################\n",
    "def get_unet_mask(image, unet_model):\n",
    "    \"\"\"\n",
    "    Ajustado para un modelo que espera (None, 128, 128, 3).\n",
    "    image: Frame con 3 canales (BGR) de OpenCV.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Convertimos a float32\n",
    "    img_float = image.astype(np.float32)\n",
    "\n",
    "    # 2) Redimensionamos a 128x128\n",
    "    #    ¡Ojo! Ajusta a las dimensiones exactas que usa tu modelo: 128 x 128\n",
    "    img_resized = cv2.resize(img_float, (128, 128))\n",
    "\n",
    "    # 3) Normalizamos si tu modelo se entrenó en [0,1]\n",
    "    img_resized /= 255.0\n",
    "\n",
    "    # 4) Expandimos dimensiones para el batch => (1, 128, 128, 3)\n",
    "    img_input = np.expand_dims(img_resized, axis=0)\n",
    "\n",
    "    # 5) Inferencia con el modelo\n",
    "    seg_pred = unet_model.predict(img_input)  \n",
    "    #    seg_pred podría tener forma (1, 128, 128, 1) ó (1, 128, 128, X)\n",
    "\n",
    "    # 6) Quitamos la dimensión de batch => (128, 128, canales_salida)\n",
    "    mask = np.squeeze(seg_pred)\n",
    "\n",
    "    # 7) Si el modelo produce más de un canal de salida (p.ej. 2 clases),\n",
    "    #    escoge el canal que te interese. Aquí, por simplicidad, canal 0:\n",
    "    if mask.ndim == 3 and mask.shape[2] > 1:\n",
    "        mask = mask[..., 0]\n",
    "\n",
    "    # 8) Binarizamos => 0/1\n",
    "    mask = np.round(mask)\n",
    "\n",
    "    # 9) Redimensionar la máscara de vuelta al tamaño original de la imagen\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    mask = cv2.resize(mask, (orig_w, orig_h))\n",
    "\n",
    "    # 10) Convertir a booleano (opcional, si quieres manipularlo como máscara)\n",
    "    mask = mask.astype(bool)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "###############################################\n",
    "#   2) FUNCIÓN PARA OBTENER ROI CON YOLO     #\n",
    "###############################################\n",
    "def get_max_yolo_roi(video_path, yolo_model, margin=15):\n",
    "    \"\"\"\n",
    "    Recorre todo el video y obtiene la ROI que cubre todas las detecciones YOLO.\n",
    "    Devuelve las coordenadas (x1, y1, x2, y2) de la ROI con margen.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    min_x1, min_y1 = float('inf'), float('inf')\n",
    "    max_x2, max_y2 = 0, 0\n",
    "\n",
    "    max_roi_with_margin = None  # Por si no se detecta nada\n",
    "    max_img = None\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Inferencia con YOLO\n",
    "        results = yolo_model(frame)\n",
    "        # YOLO >= 8.0: results[0].boxes / YOLO <8.0: results.xyxy[0]\n",
    "        if results[0].boxes is not None and len(results[0].boxes) > 0:\n",
    "            for box in results[0].boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().int().numpy()\n",
    "\n",
    "                # Ajustar coordenadas a los límites del frame\n",
    "                x1 = max(0, min(x1, frame.shape[1] - 1))\n",
    "                y1 = max(0, min(y1, frame.shape[0] - 1))\n",
    "                x2 = max(0, min(x2, frame.shape[1] - 1))\n",
    "                y2 = max(0, min(y2, frame.shape[0] - 1))\n",
    "\n",
    "                # Actualizar min y max para la ROI\n",
    "                min_x1 = min(min_x1, x1)\n",
    "                min_y1 = min(min_y1, y1)\n",
    "                max_x2 = max(max_x2, x2)\n",
    "                max_y2 = max(max_y2, y2)\n",
    "\n",
    "                max_img = frame.copy()\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Agregar márgenes asegurándonos de no salirnos de la imagen\n",
    "    if max_img is not None:\n",
    "        min_x1 = max(0, min_x1 - margin)\n",
    "        min_y1 = max(0, min_y1 - margin)\n",
    "        max_x2 = min(max_img.shape[1] - 1, max_x2 + margin)\n",
    "        max_y2 = min(max_img.shape[0] - 1, max_y2 + margin)\n",
    "        max_roi_with_margin = (min_x1, min_y1, max_x2, max_y2)\n",
    "    else:\n",
    "        print(\"No se detectó ninguna ROI en las imágenes del video.\")\n",
    "\n",
    "    return max_roi_with_margin\n",
    "\n",
    "###############################################\n",
    "#   3) FUNCIÓN PARA FILTRAR LA MÁSCARA UNET  #\n",
    "###############################################\n",
    "def filter_unet_mask_with_yolo(unet_mask, roi):\n",
    "    \"\"\"\n",
    "    Filtra la máscara de UNet para mantener solo las áreas dentro de la ROI (x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = roi\n",
    "    filtered_mask = np.zeros_like(unet_mask)\n",
    "    filtered_mask[y1:y2, x1:x2] = unet_mask[y1:y2, x1:x2]\n",
    "    return filtered_mask\n",
    "\n",
    "##################################################\n",
    "#   4) PROCESAR VIDEO, OBTENER SEGMENTACIÓN      #\n",
    "#      Y DIBUJAR CONTORNOS                       #\n",
    "##################################################\n",
    "def process_and_create_segmented_video_with_contours(\n",
    "    video_path,\n",
    "    yolo_model,\n",
    "    unet_model,\n",
    "    output_video_path,\n",
    "    margin=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Lee frame a frame el video:\n",
    "      - Obtiene la ROI global con YOLO (get_max_yolo_roi).\n",
    "      - Para cada frame, aplica get_unet_mask y filtra con filter_unet_mask_with_yolo.\n",
    "      - Dibuja contornos y guarda el nuevo video en output_video_path.\n",
    "    \"\"\"\n",
    "    # 1) Calcular ROI con YOLO\n",
    "    roi = get_max_yolo_roi(video_path, yolo_model, margin)\n",
    "\n",
    "    # Abrimos el video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Configurar writer de video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame_video = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # 2) Obtener la máscara de SERUNet\n",
    "        unet_mask = get_unet_mask(frame_video, unet_model)\n",
    "\n",
    "        # 3) Filtrar la máscara con la ROI\n",
    "        if roi is not None:\n",
    "            filtered_mask = filter_unet_mask_with_yolo(unet_mask, roi)\n",
    "        else:\n",
    "            # Si no hay ROI detectada, la máscara queda igual\n",
    "            filtered_mask = unet_mask\n",
    "\n",
    "        # 4) Convertir a uint8 y buscar contornos\n",
    "        filtered_mask_resized = filtered_mask.astype(np.uint8)\n",
    "        contours, _ = cv2.findContours(filtered_mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # 5) Dibujar los contornos en el frame original\n",
    "        frame_with_contours = frame_video.copy()\n",
    "        cv2.drawContours(frame_with_contours, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "        # 6) Agregar frame al video de salida\n",
    "        output_video.write(frame_with_contours)\n",
    "\n",
    "        frame_count += 1\n",
    "        # Liberar memoria ocasionalmente\n",
    "        if frame_count % 100 == 0:\n",
    "            gc.collect()\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    cap.release()\n",
    "    output_video.release()\n",
    "\n",
    "    print(f\"Video con segmentación y contornos creado en: {output_video_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e154a8-3417-42a9-b31f-d1c3bdc8fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/home/vplab-2024/Desktop/VoiceLab/segmentation_glottis/datasets/videos_VPLab/FN001.avi\"\n",
    "output_video_path = \"/home/vplab-2024/Desktop/VoiceLab/segmentation_glottis/models/videos/FN001_s3ar-yolo_roi1_1.mp4\"\n",
    "yolo_model_path = \"//home/vplab-2024/Desktop/VoiceLab/segmentation_glottis/models/YOLO/YOLOV8/best_yolov8n-seg-1cls.pt\"\n",
    "unet_model_path = \"/home/vplab-2024/Desktop/VoiceLab/segmentation_glottis/models/UNets/S3AR-UNet/s3ar_unet/model/SeARUNet-2/SeARUNet-2.h5\"\n",
    "\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "unet_model = load_model(unet_model_path, compile=False)\n",
    "\n",
    "process_and_create_segmented_video_with_contours(\n",
    "    video_path=video_path,\n",
    "    yolo_model=yolo_model,\n",
    "    unet_model=unet_model,\n",
    "    output_video_path=output_video_path,\n",
    "    margin=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd852f1-4914-4f4b-8f69-927f346a5f27",
   "metadata": {},
   "source": [
    "## Segmentación directa sobre ROI recortada con YOLO\n",
    "\n",
    "**Proceso del código:**\n",
    "\n",
    "1) Obtiene una **región de interés (ROI) global** recorriendo todo el video con **YOLO**.  \n",
    "2) **Recorta cada frame** del video usando la ROI obtenida con YOLO.  \n",
    "3) Aplica **UNet únicamente sobre la imagen recortada**.  \n",
    "4) Dibuja los contornos de la segmentación sobre el **video recortado**, que es el video de salida final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a58f3-422b-4bdd-aac7-cce135fce0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_s3arunet(\n",
    "    input_video_path,\n",
    "    s3arunet_path,\n",
    "    yolo_model_path,\n",
    "    output_video_path,\n",
    "    margin=15,\n",
    "    model_input_size=(128, 128)\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa un video usando YOLO para hallar una ROI global y luego un modelo S3ARUnet\n",
    "    para segmentar la parte recortada del video. El resultado se guarda como video con\n",
    "    contornos dibujados.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    - input_video_path : str\n",
    "        Ruta del video de entrada.\n",
    "    - s3arunet_path : str\n",
    "        Ruta del modelo S3ARUnet (archivo .h5).\n",
    "    - yolo_model_path : str\n",
    "        Ruta del modelo YOLO (ej. un .pt entrenado).\n",
    "    - output_video_path : str\n",
    "        Ruta de salida para el video segmentado.\n",
    "    - margin : int, opcional (default=15)\n",
    "        Margen de píxeles alrededor de la ROI detectada.\n",
    "    - model_input_size : tuple(int, int), opcional (default=(128,128))\n",
    "        Tamaño (ancho, alto) que necesita el modelo S3ARUnet.\n",
    "        Ajustar a (width, height) según tu entrenamiento.\n",
    "    \"\"\"\n",
    "\n",
    "    #################################################\n",
    "    # 1) Función auxiliar: Dibujar contornos en BGR #\n",
    "    #################################################\n",
    "    def draw_contours_on_image(mask, img_bgr, contour_color=(0, 255, 0), contour_thickness=3):\n",
    "        \"\"\"\n",
    "        Encuentra contornos en 'mask' (binaria) y los dibuja en la imagen 'img_bgr'.\n",
    "        \"\"\"\n",
    "        contours, _ = cv2.findContours(\n",
    "            mask.astype(np.uint8),\n",
    "            cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "        # Asegurar que la imagen sea BGR\n",
    "        if len(img_bgr.shape) == 2 or img_bgr.shape[2] == 1:\n",
    "            img_with_contours = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)\n",
    "        else:\n",
    "            img_with_contours = img_bgr.copy()\n",
    "\n",
    "        cv2.drawContours(\n",
    "            img_with_contours,\n",
    "            contours,\n",
    "            -1,\n",
    "            contour_color,\n",
    "            contour_thickness\n",
    "        )\n",
    "        return img_with_contours\n",
    "\n",
    "    #################################################\n",
    "    # 2) Función auxiliar: Obtener ROI global YOLO  #\n",
    "    #################################################\n",
    "    def get_max_yolo_roi(video_path, yolo_model, margin=15):\n",
    "        \"\"\"\n",
    "        Recorre todo el video para determinar la bounding box global\n",
    "        que cubra todas las detecciones YOLO.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        min_x1, min_y1 = float('inf'), float('inf')\n",
    "        max_x2, max_y2 = 0, 0\n",
    "        max_img = None\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            results = yolo_model(frame)\n",
    "            if results[0].boxes is not None and len(results[0].boxes) > 0:\n",
    "                for box in results[0].boxes:\n",
    "                    xA, yA, xB, yB = box.xyxy[0].cpu().int().numpy()\n",
    "\n",
    "                    # Limitar a los bordes de la imagen\n",
    "                    xA = max(0, min(xA, frame.shape[1] - 1))\n",
    "                    yA = max(0, min(yA, frame.shape[0] - 1))\n",
    "                    xB = max(0, min(xB, frame.shape[1] - 1))\n",
    "                    yB = max(0, min(yB, frame.shape[0] - 1))\n",
    "\n",
    "                    # Actualizar min/max\n",
    "                    min_x1 = min(min_x1, xA)\n",
    "                    min_y1 = min(min_y1, yA)\n",
    "                    max_x2 = max(max_x2, xB)\n",
    "                    max_y2 = max(max_y2, yB)\n",
    "\n",
    "                    max_img = frame.copy()\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if max_img is not None:\n",
    "            # Agregar margen\n",
    "            min_x1 = max(0, min_x1 - margin)\n",
    "            min_y1 = max(0, min_y1 - margin)\n",
    "            max_x2 = min(max_img.shape[1] - 1, max_x2 + margin)\n",
    "            max_y2 = min(max_img.shape[0] - 1, max_y2 + margin)\n",
    "            return (min_x1, min_y1, max_x2, max_y2)\n",
    "        else:\n",
    "            print(\"No se detectó ninguna ROI en el video.\")\n",
    "            return (0, 0, 0, 0)\n",
    "\n",
    "    ###################################################################\n",
    "    # 3) Función auxiliar: Redimensionar y normalizar para el modelo  #\n",
    "    ###################################################################\n",
    "    def resize_frame_to_model(cropped_frame, target_size):\n",
    "        \"\"\"\n",
    "        Redimensiona (y normaliza a [0..1]) la imagen recortada para adecuarla\n",
    "        a la entrada del modelo S3ARUnet.\n",
    "        \"\"\"\n",
    "        width, height = target_size  # target_size = (128, 128) => (W, H)\n",
    "        resized_frame = cv2.resize(cropped_frame, (width, height))\n",
    "        # Convertir a float en [0..1]\n",
    "        resized_frame = resized_frame.astype(np.float32) / 255.0\n",
    "        return np.expand_dims(resized_frame, axis=0)  # (1, H, W, 3)\n",
    "\n",
    "    #################################################\n",
    "    #   4) Comienza la lógica principal del pipeline\n",
    "    #################################################\n",
    "\n",
    "    # a) Cargamos los modelos\n",
    "    print(\"Cargando modelo YOLO...\")\n",
    "    yolo_model = YOLO(yolo_model_path)\n",
    "\n",
    "    print(\"Cargando modelo S3ARUnet...\")\n",
    "    s3ar_unet = load_model(\n",
    "        s3arunet_path,\n",
    "        compile=False,\n",
    "        custom_objects={\"InstanceNormalization\": tfa.layers.InstanceNormalization}\n",
    "    )\n",
    "\n",
    "    # b) Determinamos la ROI global con YOLO\n",
    "    x1, y1, x2, y2 = get_max_yolo_roi(input_video_path, yolo_model, margin=margin)\n",
    "    print(f\"ROI global con margen: (x1={x1}, y1={y1}, x2={x2}, y2={y2})\")\n",
    "\n",
    "    # c) Abrimos el video y configuramos el writer de salida\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    out = None\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    print(\"Procesando frames...\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # d) Recortar el frame con la ROI\n",
    "        cropped_frame = frame[y1:y2, x1:x2]\n",
    "        if cropped_frame.size == 0:\n",
    "            # ROI inválida o no detectada\n",
    "            # Se puede usar frame completo como fallback\n",
    "            cropped_frame = frame\n",
    "\n",
    "        # e) Redimensionar al tamaño que espera el modelo\n",
    "        input_batch = resize_frame_to_model(cropped_frame, model_input_size)\n",
    "\n",
    "        # f) Inferencia con S3ARUnet\n",
    "        seg_pred = s3ar_unet.predict(input_batch)\n",
    "        # seg_pred => (1, H, W, 1) o similar\n",
    "        seg_pred = np.squeeze(seg_pred)  # => (H, W) o (H, W, C)\n",
    "\n",
    "        # Si el modelo tiene varios canales de salida y solo quieres el primero:\n",
    "        # if seg_pred.ndim == 3 and seg_pred.shape[-1] > 1:\n",
    "        #     seg_pred = seg_pred[..., 0]\n",
    "\n",
    "        # g) Binarizar\n",
    "        mask_binary = (seg_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "        # h) Redimensionar la máscara a las dimensiones del recorte original\n",
    "        h_c, w_c = cropped_frame.shape[:2]\n",
    "        mask_restored = cv2.resize(\n",
    "            mask_binary,\n",
    "            (w_c, h_c),  # ancho, alto\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        )\n",
    "\n",
    "        # i) Dibujar contornos sobre el recorte\n",
    "        img_with_contours = draw_contours_on_image(\n",
    "            mask=mask_restored,\n",
    "            img_bgr=cropped_frame,\n",
    "            contour_color=(0, 255, 0),\n",
    "            contour_thickness=2\n",
    "        )\n",
    "\n",
    "        # j) Inicializar el writer de video si no se ha hecho\n",
    "        if out is None:\n",
    "            h_out, w_out = img_with_contours.shape[:2]\n",
    "            out = cv2.VideoWriter(output_video_path, fourcc, fps, (w_out, h_out))\n",
    "\n",
    "        # k) Escribir el frame en el video\n",
    "        out.write(img_with_contours)\n",
    "        frame_count += 1\n",
    "\n",
    "    # l) Liberar recursos\n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "\n",
    "    print(f\"Proceso finalizado. Se creó el video: {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51061c-21d1-4331-b071-489cac490db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_s3arunet(\n",
    "    input_video_path,\n",
    "    s3arunet_path,\n",
    "    yolo_model_path,\n",
    "    output_video_path,\n",
    "    margin=15,\n",
    "    model_input_size=(128, 128)\n",
    "):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
